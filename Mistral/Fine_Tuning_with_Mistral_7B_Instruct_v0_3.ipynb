{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code Reference:\n",
        "\n",
        "https://colab.research.google.com/drive/1BdZEGS_QsG0yHHD8BagnP3BoDVy8EHuu\n",
        "https://colab.research.google.com/drive/1bmZAiYkkeNKjVLTsNAa864h7hw_rmMnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOyVaq6r3oVB"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q -U bitsandbytes==0.42.0 peft==0.8.2 trl==0.7.10 accelerate==0.27.1 datasets==2.17.0 transformers==4.38.2\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9-uA_NfcHUu"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"ehovy/race\",\"all\")\n",
        "train_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pslpCeOVItFc"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m4CKCIpbLvr"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbQ2_dGzbOh0"
      },
      "outputs": [],
      "source": [
        "# Tokenize the dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load the Mistral model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             low_cpu_mem_usage=True,\n",
        "                                             torch_dtype=\"auto\",\n",
        "                                             device_map={\"\":0})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ZB1aRFcuuX"
      },
      "source": [
        "Format the prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmjLlUmUbgqO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import textwrap\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "\n",
        "def generate(input_text, system_prompt=\"\",max_length=512,extract_answer=False):\n",
        "    if system_prompt != \"\":\n",
        "        system_prompt = system_prompt\n",
        "    else:\n",
        "        system_prompt = \"You are a friendly and helpful assistant\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' +input_text},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages,\n",
        "                                                tokenize=False,\n",
        "                                                add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    outputs = model.generate(input_ids=inputs.to(model.device),\n",
        "                             max_new_tokens=max_length,\n",
        "                             do_sample=True,\n",
        "                             temperature=0.1,\n",
        "                             top_k=50,\n",
        "                             )\n",
        "    text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    text = text.replace('user\\n'+system_prompt+ '\\n\\n' +input_text+ '\\nmodel', '', 1)\n",
        "    wrapped_text = wrap_text(text)\n",
        "\n",
        "    if extract_answer:\n",
        "      # Regular expression to find all occurrences of 'Mistral Answer: <Word>'\n",
        "      matches = re.findall(r'Mistral Answer: (\\w+)', wrapped_text)\n",
        "      # Check if any matches were found\n",
        "      if matches:\n",
        "          # Get the last match and format the output accordingly\n",
        "          wrapped_text = \"Mistral Answer: \" + matches[-1]\n",
        "      else:\n",
        "          # Handle the case where no matches are found\n",
        "          wrapped_text = \"No Mistral Answer found\"\n",
        "\n",
        "    return wrapped_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB48hhIxczRf"
      },
      "source": [
        "## Zero shot without fune-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRYvFlzQbwIV"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "generate(\"\"\"{'article': 'Last week I talked with some of my students about what they wanted to do after they graduated,\n",
        "        and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors,\n",
        "        I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\".\n",
        "        \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\"\n",
        "        one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the\n",
        "        knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height.\n",
        "        \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows,\n",
        "        you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put\n",
        "        myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles,\n",
        "        as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" ,\n",
        "        and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images\n",
        "        of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics\n",
        "        fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter\n",
        "        what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.',\n",
        "        'question': 'We can know from the passage that the author works as a_.',\n",
        "        'options': '['doctor', 'model', 'teacher', 'reporter']'}\"\"\",\n",
        "        system_prompt=\"\"\"Image you are a student tasked with a reading comprehension exercise,\n",
        "        you are required to select the correct answer from the options for the question based on the article below.\n",
        "        Avoid providing any additional information or explanations in your response.\n",
        "        You should strictly follow the format for your response:\n",
        "        Example 1: 'Mistral Answer: doctor'.\n",
        "        Example 2: 'Mistral Answer: model'.\"\"\",\n",
        "        extract_answer=True,\n",
        "        max_length=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbPU7dhVdFzt"
      },
      "source": [
        "## Start to fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO5JdF-rcdOz"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"\"\"Content of Example: {{\n",
        "    'article': '{example[\"article\"]}',\n",
        "    'question': '{example[\"question\"]}',\n",
        "    'options': '{example[\"options\"]}'\n",
        "    }}\n",
        "\n",
        "    Return of Example:\n",
        "    'Mistral Answer': '{example[\"answer\"]}'\"\"\"\n",
        "    return [text]\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
        "\n",
        "# Define the LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r = 8,\n",
        "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"].select(range(train_size)),\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=20,\n",
        "        max_steps=100,\n",
        "        learning_rate=1e-5,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=formatting_func\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D83UFiOndT01"
      },
      "source": [
        "## After tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67idkmVvdQPs"
      },
      "outputs": [],
      "source": [
        "def format_dataset(current_traindataset):\n",
        "    # Format the output string as specified\n",
        "    formatted_text = \"\\n{{'article': '{article}',\\n'question': '{question}',\\n'options': {options}}}\\n\".format(\n",
        "        article=current_traindataset[\"article\"].replace(\"\\n\", \"\\\\n\"),\n",
        "        question=current_traindataset[\"question\"],\n",
        "        options=current_traindataset[\"options\"]\n",
        "    )\n",
        "    return formatted_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NlAnMHsdY5u"
      },
      "outputs": [],
      "source": [
        "system_prompt=\"\"\"Image you are a student tasked with a reading comprehension exercise,\n",
        "         you are required to select the correct answer from the options for the question based on the article below.\n",
        "         Note the following response format: if the first option is correct in the given options, return 'Mistral Answer: A'.\n",
        "         If the second option is correct, return 'Mistral Answer: B'.\n",
        "         If the third option is correct, return 'Mistral Answer: C'.\n",
        "         If the fourth option is correct, return 'Mistral Answer: D', and so forth for subsequent options.\n",
        "         Avoid providing any additional information or explanations in your response.\n",
        "         You should strictly follow the format for your response:\n",
        "         Example 1: 'Mistral Answer: A'.\n",
        "         Example 2: 'Mistral Answer: D'.\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMbeh6EWdZ9B"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "return_answer = {}\n",
        "dataset_type = \"test\"\n",
        "num_sample = 3000\n",
        "\n",
        "for i in tqdm(range(len(dataset[dataset_type].select(range(num_sample))))):\n",
        "  current_data = dataset[dataset_type][i]\n",
        "  input_prompt = format_dataset(dataset[dataset_type][i])\n",
        "  response = generate(input_prompt,\n",
        "         system_prompt=system_prompt,\n",
        "         extract_answer=True,\n",
        "         max_length=16)\n",
        "  return_answer[i] = {'Mistral_Answer': response, 'example_id': current_data['example_id'],\n",
        "                      'article': current_data['article'], 'answer': current_data['answer'],\n",
        "                      'question': current_data['question'], 'options': current_data['options']}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6zJAP72dbEW",
        "outputId": "ea84c8dc-0fc1-419c-ba2c-dd82097d7075"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Mistral_Answer': 'B',\n",
              " 'example_id': 'high6268.txt',\n",
              " 'article': 'There is probably no field of human activity in which our values and lifestyles are shown more clearly and strongly than they are in the clothes that we choose to wear.The dress of an individual is a kind of \"sign language\" that communicates a set of information and is usually the basis on which immediate impressions are formed.Traditionally,a concern for clothes was considered to be an affair of females,while men took pride in the fact that they were completely lacking in clothes consciousness .\\nThis type of American culture is by degrees changing as man dress takes on greater variety and color.Even as early as 1955,a researcher in Michigan said that _ White collar workers in particular viewed dress as a symbol of ability,which could be used to impress or influence others,especially in the work situation.The white collar worker was described as extremely concerned about the impression his clothing made on his superiors .Although blue collar workers were less aware that they might be judged on the basis of their clothing,they recognized that any difference from the accepted pattern of dress would be made fun of by fellow workers.\\nSince that time,of course,the patterns have changed:the typical office worker may now be wearing the blue shirt,and the laborer a white shirt;but the importance of dress has not become less.Other researchers in recent years have helped to prove its importance in the lives of individuals at various age levels and in different social and economic status groups .',\n",
              " 'answer': 'B',\n",
              " 'question': 'The passage tells us that   _  .',\n",
              " 'options': ['our values and lifestyles are in no field of human activity',\n",
              "  'the clothes that we choose to wear have something to do with our values and lifestyles',\n",
              "  'our values and lifestyles are from the sign language',\n",
              "  'the clothes we choose to wear depend on a set of information and immediate impression']}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "return_answer[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7xPETVQEd2O"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_answer(text, options):\n",
        "    \"\"\"\n",
        "    Extracts the answer from a given text string formatted as 'Mistral Answer: <answer>'.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text containing the answer.\n",
        "    options (list): The list of options to match the answer against.\n",
        "\n",
        "    Returns:\n",
        "    str: The extracted answer as a single letter (A, B, C, or D) if matched, otherwise None.\n",
        "    \"\"\"\n",
        "    match = re.search(r'Mistral Answer:\\s*(\\w+)', text)\n",
        "    if match:\n",
        "        answer = match.group(1)\n",
        "        if len(answer) == 1:\n",
        "            return answer\n",
        "        else:\n",
        "            for i, option in enumerate(options):\n",
        "                if answer == option:\n",
        "                    return chr(65 + i)  # Convert index to corresponding letter (A, B, C, or D)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPcgkjZqEfkW"
      },
      "outputs": [],
      "source": [
        "for k,v in return_answer.items():\n",
        "  v[\"Mistral_Answer\"] = extract_answer(v[\"Mistral_Answer\"],v[\"options\"])\n",
        "\n",
        "return_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3gUqeYfEj8J"
      },
      "outputs": [],
      "source": [
        "def get_acc(answer):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for k,v in return_answer.items():\n",
        "    if v[\"Mistral_Answer\"]:\n",
        "      total += 1\n",
        "      if v[\"Mistral_Answer\"] == v[\"answer\"]:\n",
        "        correct += 1\n",
        "  print(total)\n",
        "  return round(correct/total,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfLopLgmEl4i"
      },
      "outputs": [],
      "source": [
        "acc = get_acc(return_answer)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oIPa0UgdcFG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Write the dictionary to a JSON file\n",
        "with open(f'fine_tuned_{train_size}_with_mistral-7b-instruct-v0.3_answers.json', 'w') as f:\n",
        "    json.dump(return_answer, f, indent=4)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
