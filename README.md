# Evaluate LLM’s performance in reading-comprehension tasks

This project tests LLM understanding of complex reading materials and their ability to answer exam-level questions. 

## Introduction
This project evaluates and compares [Llama-3](https://llama.meta.com/llama3/) and [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)'s ability on reading-comprehension tasks. We used [RACE](https://huggingface.co/datasets/ehovy/race), a dataset for benchmark evaluation of methods in the reading comprehension task, as our dataset.
The experiment is conducted in four scenarios: zero-shot, one-shot, three-shot and fine-tuned. Accuracy is calculated as a metric to compare two model's performance.

## Code and output

[Mistral](https://github.com/riboyuan99/CS263_Final_Project/tree/main/Mistral) contains code for our experiment under different settings, namely zero-shot, one-shot, three-shot and fine-tuned. It also contains [output](https://github.com/riboyuan99/CS263_Final_Project/tree/main/Mistral/output_data) generated by Mistral.

[Llama 3](https://github.com/riboyuan99/CS263_Final_Project/tree/main/Llama%203) contains code for our experiment under different settings, namely zero-shot, one-shot, three-shot and fine-tuned. It also contains [output](https://github.com/riboyuan99/CS263_Final_Project/tree/main/Llama%203/output_data) generated by Llama-3.

One data point in output files may look like this:
```json
{
    "0": {
        "Mistral_Answer": "A",
        "example_id": "high19432.txt",
        "article": "The rain had continued for a week and the flood had created a big river which were running by Nancy Brown's farm. As she tried to gather her cows to a higher ground, she slipped and hit her head on a fallen tree trunk. The fall made her unconscious for a moment or two. When she came to, Lizzie, one of her oldest and favorite cows, was licking her face.",
        "answer": "C",
        "question": "What did Nancy try to do before she fell over?",
        "options": [
            "Measure the depth of the river",
            "Look for a fallen tree trunk",
            "Protect her cows from being drowned",
            "Run away from the flooded farm"
        ]
    }
}
```



## Method

The flowchart below outlines the high-level method for the project.
![Alt Text](https://github.com/riboyuan99/CS263_Final_Project/blob/main/263_Method_Graph.png)


## Result

|  | Zero-shot | One-shot | Three-shot | Fine-tuned |
|----------|----------|----------|----------|----------|
| Llama-3    | 61%   | 64%   | 63%   | 61%   |
| Mistral    | 56%   | 58%   | 60%   | 56%   |

## Work distribution

This project is done by Ribo Yuan, Zhihan Chen, and Hongyi Qi.

Ribo Yuan is in charge of dataset selection and evaluation pipeline. He ensured that the chosen dataset is representative of the task and implemented the necessary steps to evaluate model performance and analyze the results.

Zhihan Chen is in charge of setting up and optimizing workflows for Mis- tral 7B, with a focus on system configuration, integration, and assessing performance.

Hongyi Qi is in charge of set-ting up, integrating, and evaluating the perfor-mance of the latest version of Meta Llama–Llama3.
