# Evaluate LLMâ€™s performance in reading-comprehension tasks

This project tests LLM understanding of complex reading materials and their ability to answer exam-level questions. 

## Introduction
This project evaluates and compares [Llama-3](https://llama.meta.com/llama3/) and [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)'s ability on reading-comprehension tasks. We used [RACE](https://huggingface.co/datasets/ehovy/race), a dataset for benchmark evaluation of methods in the reading comprehension task, as our dataset.
The experiment is conducted in four scenarios: zero-shot, one-shot, three-shot and fine-tuned. 
## Method

The flowchart below outlines the high-level method for this project.

